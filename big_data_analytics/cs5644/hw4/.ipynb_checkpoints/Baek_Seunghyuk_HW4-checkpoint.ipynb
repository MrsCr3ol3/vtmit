{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.tree\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('OnlineNewsPopularity.csv','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = list()\n",
    "\n",
    "for line in file:\n",
    "    newLine = line.rstrip()\n",
    "    x = np.array(newLine.split(\",\"))\n",
    "    filelist.append(x)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://mashable.com/2013/01/07/amazon-instant-video-browser/',\n",
       "       ' 731.0', ' 12.0', ' 219.0', ' 0.663594466988', ' 0.999999992308',\n",
       "       ' 0.815384609112', ' 4.0', ' 2.0', ' 1.0', ' 0.0', ' 4.6803652968',\n",
       "       ' 5.0', ' 0.0', ' 1.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0',\n",
       "       ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.0',\n",
       "       ' 496.0', ' 496.0', ' 496.0', ' 1.0', ' 0.0', ' 0.0', ' 0.0',\n",
       "       ' 0.0', ' 0.0', ' 0.0', ' 0.0', ' 0.500331204081',\n",
       "       ' 0.378278929586', ' 0.0400046751006', ' 0.0412626477296',\n",
       "       ' 0.0401225435029', ' 0.521617145481', ' 0.0925619834711',\n",
       "       ' 0.0456621004566', ' 0.013698630137', ' 0.769230769231',\n",
       "       ' 0.230769230769', ' 0.378636363636', ' 0.1', ' 0.7', ' -0.35',\n",
       "       ' -0.6', ' -0.2', ' 0.5', ' -0.1875', ' 0.0', ' 0.1875', ' 593'],\n",
       "      dtype='|S60')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filearray = np.array(filelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39645L, 61L)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filearray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filearray[1:,:-1]\n",
    "y = filearray[1:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 593' ' 711' ' 1500' ... ' 1900' ' 1100' ' 1300']\n"
     ]
    }
   ],
   "source": [
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [x.strip(' ') for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [float(x) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [1 if x >1400 else 0 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted:', array([0, 0]))\n",
      "('truth', array([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted:\", mlp_clf.predict(data[-2:,1:].astype(float)))\n",
    "print(\"truth\", Y[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-1 Use 2 hidden layers with 20 nodes in each layer\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20))\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 10.708644951557833)\n",
      "('Number of Layers:', 4)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 20L), (20L, 20L), (20L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-2 Use 2 hidden layers with 20 nodes in each layer, tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20), activation = 'tanh')\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.6914463112885956)\n",
      "('Number of Layers:', 4)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 20L), (20L, 20L), (20L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2-1 Use 2 hidden layers with 100 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100))\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 10.768781244523826)\n",
      "('Number of Layers:', 4)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 100L), (100L, 100L), (100L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2-2 Use 2 hidden layers with 100 nodes in each layer. tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100), activation = 'tanh')\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.6902934092321684)\n",
      "('Number of Layers:', 4)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 100L), (100L, 100L), (100L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20, 20, 20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3-1 Use 5 hidden layers with 20 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20))\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.7061767349969924)\n",
      "('Number of Layers:', 7)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 20L), (20L, 20L), (20L, 20L), (20L, 20L), (20L, 20L), (20L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20, 20, 20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3-2 Use 5 hidden layers with 20 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20), activation = 'tanh')\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.6932473980527416)\n",
      "('Number of Layers:', 7)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 20L), (20L, 20L), (20L, 20L), (20L, 20L), (20L, 20L), (20L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100, 100, 100),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4-1 Use 5 hidden with 100 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100))\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 5.91001618872255)\n",
      "('Number of Layers:', 7)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 100L), (100L, 100L), (100L, 100L), (100L, 100L), (100L, 100L), (100L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100, 100, 100),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4-2 Use 5 hidden with 100 nodes in each layer. tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100), activation = 'tanh')\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.6920912550769159)\n",
      "('Number of Layers:', 7)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 100L), (100L, 100L), (100L, 100L), (100L, 100L), (100L, 100L), (100L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(data[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.56      0.51      3525\n",
      "          1       0.58      0.49      0.53      4404\n",
      "\n",
      "avg / total       0.53      0.52      0.52      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.01      0.02      4423\n",
      "          1       0.44      0.99      0.61      3506\n",
      "\n",
      "avg / total       0.56      0.44      0.28      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.18      0.28      3733\n",
      "          1       0.55      0.89      0.68      4196\n",
      "\n",
      "avg / total       0.57      0.55      0.49      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.99      0.66      3919\n",
      "          1       0.75      0.02      0.04      4010\n",
      "\n",
      "avg / total       0.62      0.50      0.35      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.01      0.03      4482\n",
      "          1       0.43      0.98      0.60      3446\n",
      "\n",
      "avg / total       0.48      0.44      0.28      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1-1 Use 2 hidden layers with 20 nodes in each layer\n",
    "datax = data[:,1:]\n",
    "Y = np.array(Y)\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20))\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.88      0.59      3525\n",
      "          1       0.58      0.13      0.21      4404\n",
      "\n",
      "avg / total       0.52      0.46      0.38      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.95      0.71      4423\n",
      "          1       0.54      0.07      0.12      3506\n",
      "\n",
      "avg / total       0.55      0.56      0.45      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.86      0.61      3733\n",
      "          1       0.57      0.17      0.26      4196\n",
      "\n",
      "avg / total       0.53      0.49      0.43      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.41      0.42      3919\n",
      "          1       0.46      0.50      0.48      4010\n",
      "\n",
      "avg / total       0.45      0.45      0.45      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.81      0.68      4482\n",
      "          1       0.52      0.27      0.35      3446\n",
      "\n",
      "avg / total       0.56      0.57      0.54      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1-2 Use 2 hidden layers with 20 nodes in each layer, tanh\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20), activation = 'tanh')\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.83      0.58      3525\n",
      "          1       0.55      0.16      0.25      4404\n",
      "\n",
      "avg / total       0.50      0.46      0.40      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      4423\n",
      "          1       0.44      1.00      0.61      3506\n",
      "\n",
      "avg / total       0.20      0.44      0.27      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.01      0.02      3733\n",
      "          1       0.53      0.99      0.69      4196\n",
      "\n",
      "avg / total       0.47      0.53      0.37      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.98      0.66      3919\n",
      "          1       0.60      0.03      0.05      4010\n",
      "\n",
      "avg / total       0.55      0.50      0.35      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.56      0.61      4482\n",
      "          1       0.53      0.65      0.58      3446\n",
      "\n",
      "avg / total       0.61      0.60      0.60      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2-1 Use 2 hidden layers with 100 nodes in each layer.\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100) )\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.19      0.27      3525\n",
      "          1       0.55      0.80      0.65      4404\n",
      "\n",
      "avg / total       0.50      0.53      0.48      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.88      0.69      4423\n",
      "          1       0.48      0.13      0.21      3506\n",
      "\n",
      "avg / total       0.52      0.55      0.48      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.96      0.64      3733\n",
      "          1       0.60      0.05      0.09      4196\n",
      "\n",
      "avg / total       0.54      0.48      0.35      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.67      0.60      3919\n",
      "          1       0.58      0.45      0.50      4010\n",
      "\n",
      "avg / total       0.56      0.56      0.55      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.92      0.71      4482\n",
      "          1       0.53      0.11      0.18      3446\n",
      "\n",
      "avg / total       0.56      0.57      0.48      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2-2 Use 2 hidden layers with 100 nodes in each layer. tanh\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100), activation = 'tanh')\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.62      0.52      3525\n",
      "          1       0.56      0.39      0.46      4404\n",
      "\n",
      "avg / total       0.51      0.49      0.48      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.52      0.58      4423\n",
      "          1       0.52      0.67      0.59      3506\n",
      "\n",
      "avg / total       0.60      0.59      0.59      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.34      0.43      3733\n",
      "          1       0.57      0.77      0.65      4196\n",
      "\n",
      "avg / total       0.57      0.57      0.55      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.98      0.66      3919\n",
      "          1       0.73      0.04      0.08      4010\n",
      "\n",
      "avg / total       0.62      0.51      0.37      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.57      0.59      4482\n",
      "          1       0.47      0.50      0.48      3446\n",
      "\n",
      "avg / total       0.54      0.54      0.54      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3-1 Use 5 hidden layers with 20 nodes in each layer.\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20) )\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.62      0.52      3525\n",
      "          1       0.57      0.40      0.47      4404\n",
      "\n",
      "avg / total       0.52      0.50      0.49      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.00      0.01      4423\n",
      "          1       0.44      1.00      0.61      3506\n",
      "\n",
      "avg / total       0.53      0.44      0.28      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.83      0.61      3733\n",
      "          1       0.58      0.21      0.30      4196\n",
      "\n",
      "avg / total       0.53      0.50      0.45      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      3919\n",
      "          1       0.51      1.00      0.67      4010\n",
      "\n",
      "avg / total       0.26      0.51      0.34      7929\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.79      0.67      4482\n",
      "          1       0.50      0.28      0.36      3446\n",
      "\n",
      "avg / total       0.55      0.57      0.54      7928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3-2 Use 5 hidden layers with 20 nodes in each layer. tnah\n",
    "\n",
    "for train_index, test_index in kf.split(datax):\n",
    "    x_train, x_test = datax[train_index], datax[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20), activation = 'tanh')\n",
    "    mlp_clf.fit(x_train.astype(np.float), y_train)\n",
    "    \n",
    "    predicted = mlp_clf.predict(x_test.astype(np.float))\n",
    "    \n",
    "    print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.5, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-1 Use 2 hidden layers with 20 nodes in each layer with different learning rate\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20), learning_rate_init = 0.5)\n",
    "mlp_clf.fit(data[:,1:].astype(np.float), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted:', array([1, 1]))\n",
      "('truth', array([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted:\", mlp_clf.predict(data[-2:,1:].astype(float)))\n",
    "print(\"truth\", Y[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Loss:', 0.6975631459484516)\n",
      "('Number of Layers:', 4)\n",
      "('Output Layer Activation:', 'logistic')\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Shape of the network weights for each layer:\n",
      "The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\n",
      "Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\n",
      "Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\n",
      "Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\n",
      "[(59L, 20L), (20L, 20L), (20L, 1L)]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Loss:\", mlp_clf.best_loss_)\n",
    "print(\"Number of Layers:\", mlp_clf.n_layers_)\n",
    "print(\"Output Layer Activation:\", mlp_clf.out_activation_)\n",
    "print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Shape of the network weights for each layer:\")\n",
    "print(\"The first tuple is basically (number of features,size of the first hidden layer), e.g. (5,10)\")\n",
    "print(\"Second tuple is (size of the first hidden layer, size of the second hidden layer), e.g. (10,4)\")\n",
    "print(\"Third tuple is (size of the second hidden layer, size of the third hidden layer), e.g. (4,5)\")\n",
    "print(\"Lastly last layer will output the result, therefore it's size is (size of the third hidden layer, 1), e.g. (5,1)\")\n",
    "print([coef.shape for coef in mlp_clf.coefs_])\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.53714214 0.47534368 0.52768319 0.49716232 0.44134712] average:  0.495735687908289\n",
      "precision:  [0.55924442 0.45870835 0.53252861 0.64622642 0.51737452] average:  0.5428164628088\n",
      "f1 score:  [0.5066336  0.61291736 0.68988337 0.67119155 0.55929332] average:  0.6079838402877199\n",
      "recall:  [0.17870118 0.0427838  0.98975214 0.99725686 0.01625073] average:  0.4449489416360457\n"
     ]
    }
   ],
   "source": [
    "#1-1 Use 2 hidden layers with 20 nodes in each layer\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20))\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.45856981 0.4448228  0.52162946 0.50573843 0.43529263] average:  0.4732106266271586\n",
      "precision:  [0.53394355 0.45105954 0.63049096 0.57725948 0.51697128] average:  0.5419449602050601\n",
      "f1 score:  [0.25278023 0.11835507 0.12442008 0.03617073 0.09225875] average:  0.12479696901850426\n",
      "recall:  [0.07970027 0.21563035 0.17159199 0.01770574 0.78554846] average:  0.2540353620947823\n"
     ]
    }
   ],
   "source": [
    "#1-2 Use 2 hidden layers with 20 nodes in each layer, tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20), activation = 'tanh')\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.55378989 0.57661748 0.52755707 0.52074663 0.63080222] average:  0.5619026561286412\n",
      "precision:  [0.57464286 0.54903877 0.57701342 0.70232558 0.60200669] average:  0.601005465033087\n",
      "f1 score:  [7.06413580e-01 5.69962952e-04 6.91907944e-01 6.46004555e-01\n",
      " 6.10191317e-01] average:  0.5310174716625726\n",
      "recall:  [0.03882834 0.9988591  0.07626311 0.45660848 0.00174115] average:  0.3144600344491399\n"
     ]
    }
   ],
   "source": [
    "#2-1 Use 2 hidden layers with 100 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100))\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.46739816 0.5506369  0.48366755 0.4715601  0.43466196] average:  0.4815849328277871\n",
      "precision:  [0.55542886 0.47148455 0.59078591 0.50593584 0.50799827] average:  0.5263266853925133\n",
      "f1 score:  [0.28350948 0.57236304 0.39138069 0.26617874 0.60571931] average:  0.4238302513058049\n",
      "recall:  [0.35217984 0.99971477 0.14823642 0.83391521 0.43731863] average:  0.5542729738168519\n"
     ]
    }
   ],
   "source": [
    "#2-2 Use 2 hidden layers with 100 nodes in each layer. tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100), activation = 'tanh')\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.51885484 0.56968092 0.56804137 0.53827721 0.44210394] average:  0.5273916535234057\n",
      "precision:  [0.57543103 0.442606   0.63338246 0.50547835 0.60888889] average:  0.5531573472873197\n",
      "f1 score:  [0.41055046 0.60268402 0.03348059 0.01281735 0.07138965] average:  0.2261844122039113\n",
      "recall:  [0.8140327  0.99515117 0.00762631 0.18354115 0.0548462 ] average:  0.4110395046733733\n"
     ]
    }
   ],
   "source": [
    "#3-1 Use 5 hidden layers with 20 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20))\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.45894816 0.53916005 0.47332577 0.49854963 0.56281534] average:  0.5065597885064458\n",
      "precision:  [0.5458256  0.44967682 0.57403009 0.58911954 0.5125    ] average:  0.5342304111152341\n",
      "f1 score:  [0.02603707 0.21541195 0.6920855  0.47563249 0.37455566] average:  0.3567445333072411\n",
      "recall:  [0.13169846 0.06446092 0.10343184 0.99975062 0.04497969] average:  0.2688643059922443\n"
     ]
    }
   ],
   "source": [
    "#3-2 Use 5 hidden layers with 20 nodes in each layer. tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20), activation = 'tanh')\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.50460335 0.44204818 0.53726826 0.49489217 0.43453582] average:  0.48266955570241904\n",
      "precision:  [0.56920502 0.47857948 0.52962669 0.510116   0.37681159] average:  0.49286775622635004\n",
      "f1 score:  [0.47135417 0.59748712 0.32319722 0.65975104 0.53421927] average:  0.5172017624298333\n",
      "recall:  [0.24137148 0.23074729 0.93755958 0.00972569 1.        ] average:  0.4838808074340252\n"
     ]
    }
   ],
   "source": [
    "#4-1 Use 5 hidden with 100 nodes in each layer.\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100))\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.46916383 0.55984361 0.51355783 0.50561231 0.43529263] average:  0.4966940419380427\n",
      "precision:  [0.55542944 0.4421743  0.55576208 0.59250676 0.50307557] average:  0.5297896290919968\n",
      "f1 score:  [0.14442013 0.61320507 0.18400964 0.02399021 0.60594338] average:  0.31431368671614585\n",
      "recall:  [0.16121708 0.92641187 0.42326025 0.08104738 0.98520023] average:  0.5154273604628227\n"
     ]
    }
   ],
   "source": [
    "#4-2 Use 5 hidden with 100 nodes in each layer. tanh\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100), activation = 'tanh')\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.55542944 0.4421743  0.47080338 0.49426157 0.43466196] average:  0.4794661284388352\n",
      "precision:  [0.         0.4421743  0.         0.50573843 0.        ] average:  0.18958254508765293\n",
      "f1 score:  [0.         0.61320507 0.69212371 0.         0.60594338] average:  0.38225443262497877\n",
      "recall:  [1. 0. 0. 1. 0.] average:  0.4\n"
     ]
    }
   ],
   "source": [
    "#task2 Use 5 hidden layers with 100 nodes in each layer.different init raite\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100), learning_rate_init = 0.5)\n",
    "\n",
    "a = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, n_jobs=-1)\n",
    "b = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'precision', n_jobs=-1)\n",
    "c = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'f1', n_jobs=-1)\n",
    "d = cross_val_score(mlp_clf, data[:,1:].astype(np.float), Y, cv=kf, scoring = 'recall', n_jobs=-1)\n",
    "\n",
    "print \"accuracy: \", a, \"average: \", np.mean(a)\n",
    "print \"precision: \",b, \"average: \", np.mean(b)\n",
    "print \"f1 score: \",c, \"average: \", np.mean(c)\n",
    "print \"recall: \",d, \"average: \", np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
