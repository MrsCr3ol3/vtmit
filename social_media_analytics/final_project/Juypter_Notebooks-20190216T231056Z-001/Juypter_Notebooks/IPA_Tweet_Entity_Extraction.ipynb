{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from tweepy import Stream\n",
    "# from tweepy import OAuthHandler\n",
    "# from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import tweepy\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Tweets saved from extract process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets: 5951\n"
     ]
    }
   ],
   "source": [
    "# Load in all of the extract files into separate data frames\n",
    "df = pd.read_csv('C:\\data\\CS5664\\IPA_Teets_ALL.csv')\n",
    "print \"Number of Tweets: \" + str(df[\"id\"].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'victortellegio', u'jayho79', u'not', u'to', u'hijack', u'but', u'maui', u'brewings', u'double', u'blood', u'orange', u'ipa', u'in', u'the', u'can', u'is', u'legit', u'as', u'hell', u'\\ud83d', u'\\udc4c', u'\\ud83c', u'\\udffb']\n"
     ]
    }
   ],
   "source": [
    "raw_tweets = df['text'].values\n",
    "exclude = set(string.punctuation)\n",
    "tokenized_tweets = []\n",
    "tokenizer = TweetTokenizer()\n",
    "for tweet_text in raw_tweets:\n",
    "    tokens = tokenizer.tokenize(tweet_text.lower())\n",
    "    tokenized_tweets.append(''.join([ch for ch in ' '.join(tokens) if ch not in exclude]).split())\n",
    "print(tokenized_tweets[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just earned the 'I Believe in IPA! (Level 4)' badge on @untappd! https://t.co/Hz8NQ0owBi #ibelieveinIPA\n",
      "[u'earned', u'believe', u'ipa', u'level', u'4', u'badge', u'untappd', u'httpstcohz8nq0owbi', u'ibelieveinipa']\n"
     ]
    }
   ],
   "source": [
    "sws = set(stopwords.words('english'))\n",
    "sws.add('rt') # Tweet specific stop-words\n",
    "sws.add(u'https\\u2026') # Tweet specific stop-words\n",
    "sws.add(u'\\u2026') \n",
    "sws.add(u'\\udea8')\n",
    "sws.add(u'\\ud83d')\n",
    "sws.add(u'\\u201c')\n",
    "sws.add(u'\\udc4c')\n",
    "sws.add(u'\\ud83c')\n",
    "sws.add(u'\\udffb')\n",
    "sws.add(u'�')\n",
    "sws.add(u'�')\n",
    "sws.add(u'—')\n",
    "sws.add(u'’')\n",
    "sws.add(u'�')\n",
    "sws_removed_tweets = []\n",
    "for j,sent in enumerate(tokenized_tweets):\n",
    "    sws_removed_tweets.append(([i for i in sent if i not in sws]))\n",
    "\n",
    "    \n",
    "#        print \"Tweet: \" + str(i) + \" Sent: \" + str(j) + \" Without - \", ' '.join([i for i in sent.lower().split() if i not in sws])\n",
    "    \n",
    "print raw_tweets[1]\n",
    "print sws_removed_tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67240\n"
     ]
    }
   ],
   "source": [
    "flattened_words = [item for sublist in sws_removed_tweets for item in sublist]\n",
    "df_words = pd.DataFrame(flattened_words)\n",
    "print df_words.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipa         4431\n",
      "drinking    1278\n",
      "untappd      630\n",
      "ale          620\n",
      "level        596\n",
      "badge        589\n",
      "earned       587\n",
      "believe      586\n",
      "pale         584\n",
      "india        518\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df_words[0].value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(sws_removed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in sws_removed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.031*\"ipa\" + 0.008*\"de\" + 0.006*\"la\"\n",
      "Topic 1: 0.060*\"ipa\" + 0.033*\"drinking\" + 0.019*\"—\"\n",
      "Topic 2: 0.062*\"ipa\" + 0.025*\"untappd\" + 0.024*\"level\"\n"
     ]
    }
   ],
   "source": [
    "res = ldamodel.print_topics(num_topics=3, num_words=3)\n",
    "for r in res:\n",
    "    print u'Topic ' + str(r[0]) + \": \" + r[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def nltk_entities(text):\n",
    "    \"\"\"\n",
    "    Extract entities using the NLTK named entity chunker.\n",
    "    \"\"\"\n",
    "    text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    results = defaultdict(list)\n",
    "    lst_results = []\n",
    "    for entity in nltk.ne_chunk(text):\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            etext = \" \".join([word for word, tag in entity.leaves()])\n",
    "            label = entity.label()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if label == 'PERSON':\n",
    "            key = 'persons'\n",
    "        elif label == 'ORGANIZATION':\n",
    "            key = 'organizations'\n",
    "        elif label == 'LOCATION':\n",
    "            key = 'locations'\n",
    "        elif label == 'GPE':\n",
    "            key = 'other'\n",
    "        else:\n",
    "            key = None\n",
    "\n",
    "        if key:\n",
    "            results[key].append(etext)\n",
    "            lst_results.append(etext)\n",
    "\n",
    "    return lst_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = []\n",
    "import unicodedata\n",
    "# look at the entities for the first few tweets\n",
    "for tweet in raw_tweets:\n",
    "    ntweet = unicodedata.normalize('NFKD', tweet.decode('unicode-escape')).encode('ascii','ignore')\n",
    "#    print nltk_entities(ntweet)\n",
    "    entity_list.append(nltk_entities(ntweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11501\n"
     ]
    }
   ],
   "source": [
    "ind_entities = [item for sublist in entity_list for item in sublist]\n",
    "df_ind_entities = pd.DataFrame(ind_entities)\n",
    "print df_ind_entities.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robotics                    10\n",
      "JmarrMarr                   10\n",
      "Google                      10\n",
      "Public                      10\n",
      "Hazy                        10\n",
      "Pilsner                     10\n",
      "Cascade                     10\n",
      "IPa                          9\n",
      "Race Commissionera           9\n",
      "SierraNevada                 9\n",
      "SenatorFifield               9\n",
      "PwC                          9\n",
      "Amager Bryghus               9\n",
      "UK                           9\n",
      "Brewery                      9\n",
      "English                      9\n",
      "BrewDog                      9\n",
      "Voodoo Ranger Juicy Haze     9\n",
      "East Coast                   9\n",
      "BellaFlokarti                9\n",
      "Craft                        9\n",
      "IntPublishers                9\n",
      "Little Johnny Howard         9\n",
      "Juicy                        9\n",
      "Session                      9\n",
      "Huell Melon                  8\n",
      "Stone                        8\n",
      "Cheers                       8\n",
      "France                       8\n",
      "APS Commissioner             8\n",
      "                            ..\n",
      "Nancy Bird                   7\n",
      "Union Jack                   7\n",
      "Nice                         7\n",
      "IIII                         7\n",
      "CALLA                        7\n",
      "Porcina                      6\n",
      "BrinkburnStBrew              6\n",
      "Tapped                       6\n",
      "BanksRC                      6\n",
      "England                      6\n",
      "Tangerine Express            6\n",
      "Dick Cantwell                6\n",
      "Red                          6\n",
      "Bodour                       6\n",
      "West                         6\n",
      "bigH_88                      6\n",
      "Morgan Begg                  6\n",
      "Alexander Keith              6\n",
      "Weave                        6\n",
      "Love                         6\n",
      "INSTITUTE of                 6\n",
      "Lagos                        6\n",
      "PUBLIC                       6\n",
      "Barncliffe Bitter            6\n",
      "Devil                        6\n",
      "Amra Mango                   6\n",
      "Pale                         6\n",
      "Northeast                    6\n",
      "Public Funds for Private     6\n",
      "Blood IPA                    6\n",
      "Name: 0, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df_ind_entities[0].value_counts()[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
